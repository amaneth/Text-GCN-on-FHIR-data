{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9140096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pyvis.network import Network\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb730e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = 'synthea_1m_fhir_3_0_May_24/output_1/fhir/'\n",
    "directory = os.fsencode(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1037eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert fhir json to pandas dataframe\n",
    "encounters,conditions,patient_ids,procedures, observations, dignotstics_reports,\\\n",
    "immunizations, medication_orders, careplans = ([] for _ in range(9))\n",
    "encounters_display,conditions_display,patient_ids_display,procedures_display, observations_display, dignotstics_reports_display,\\\n",
    "immunizations_display, medication_orders_display, careplans_display = ([] for _ in range(9))\n",
    "\n",
    "for i, file in enumerate(os.listdir(directory)):\n",
    "    filename = os.fsdecode(file)\n",
    "    with open(dataset_location+filename, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        resources = data['entry']\n",
    "        patient_id = resources[0]['resource']['id']\n",
    "        name = resources[0]['resource']['name'][0]['family']\n",
    "        patient_ids.append(patient_id)\n",
    "        patient_ids_display.append(name)\n",
    "        patient_encounters,patient_conditions, patient_procedures,patient_observations,\\\n",
    "        patient_diagnostics_reports, patient_immunizations, patient_medication_orders,\\\n",
    "        patient_careplans = (set() for _ in range(8))\n",
    "        patient_encounters_display,patient_conditions_display, patient_procedures_display,patient_observations_display,\\\n",
    "        patient_diagnostics_reports_display, patient_immunizations_display, patient_medication_orders_display,\\\n",
    "        patient_careplans_display = (set() for _ in range(8))\n",
    "        \n",
    "   \n",
    "        for resource in resources:\n",
    "            resource_type = resource['resource']['resourceType']\n",
    "            if resource_type =='Encounter':\n",
    "                patient_encounters.add(resource['resource']['type'][0]['coding'][0]['code'])\n",
    "                patient_encounters_display.add(resource['resource']['type'][0].get('text',''))\n",
    "            elif resource_type =='Condition':\n",
    "                patient_conditions.add(resource['resource']['code']['coding'][0]['code'])\n",
    "                patient_conditions_display.add(resource['resource']['code']['coding'][0]['display'])\n",
    "            elif resource_type =='Procedure':\n",
    "                code = resource['resource']['code']['coding'][0]['code']\n",
    "                display = resource['resource']['code']['coding'][0]['display']\n",
    "                #ignore documentation of current medications\n",
    "                if code!='428191000124101':\n",
    "                    patient_procedures.add(code)  \n",
    "                    patient_procedures_display.add(display) \n",
    "            elif resource_type =='Observation':\n",
    "                patient_observations.add(resource['resource']['code']['coding'][0]['code'])\n",
    "                patient_observations_display.add(resource['resource']['code']['coding'][0]['display'])\n",
    "            elif resource_type =='DiagnosticReport':\n",
    "                patient_diagnostics_reports.add(resource['resource']['code']['coding'][0]['code'])\n",
    "                patient_diagnostics_reports_display.add(resource['resource']['code']['coding'][0]['display'])\n",
    "            elif resource_type == 'Immunization':\n",
    "                patient_immunizations.add(resource['resource']['vaccineCode']['coding'][0]['code'])\n",
    "                patient_immunizations_display.add(resource['resource']['vaccineCode']['coding'][0]['display'])\n",
    "            elif resource_type == medication:\n",
    "                patient_medication_orders.add(resource['resource']['code']['coding'][0]['code'])\n",
    "                patient_medication_orders_display.add(resource['resource']['code']['coding'][0]['display'])\n",
    "            elif resource_type == 'CarePlan':\n",
    "                patient_careplans.add(resource['resource']['category'][0]['coding'][0]['code'])\n",
    "                patient_careplans_display.add(resource['resource']['category'][0]['coding'][0]['display'])\n",
    "                \n",
    "        encounters.append(','.join(patient_encounters))\n",
    "        conditions.append(','.join(patient_conditions))\n",
    "        procedures.append(','.join(patient_procedures))\n",
    "        observations.append(','.join(patient_observations))\n",
    "        dignotstics_reports.append(','.join(patient_diagnostics_reports))\n",
    "        immunizations.append(','.join(patient_immunizations))\n",
    "        medication_orders.append(','.join(patient_medication_orders))\n",
    "        careplans.append(','.join(patient_careplans))\n",
    "        \n",
    "        encounters_display.append(','.join(patient_encounters_display))\n",
    "        conditions_display.append(','.join(patient_conditions_display))\n",
    "        procedures_display.append(','.join(patient_procedures_display))\n",
    "        observations_display.append(','.join(patient_observations_display))\n",
    "        dignotstics_reports_display.append(','.join(patient_diagnostics_reports_display))\n",
    "        immunizations_display.append(','.join(patient_immunizations_display))\n",
    "        medication_orders_display.append(','.join(patient_medication_orders_display))\n",
    "        careplans_display.append(','.join(patient_careplans_display))\n",
    "#     if i==10:\n",
    "#         break\n",
    "        \n",
    "        \n",
    "data = {'patient': patient_ids,'Encounter': encounters, 'Condition': conditions, 'Procedure':procedures, 'Observation':observations, \n",
    "        'Diagnostics Report':dignotstics_reports, 'Immunization':immunizations, 'Medication Orders':medication_orders, 'Careplan': careplans,\n",
    "       'patient display': patient_ids_display,'Encounter display': encounters_display, 'Condition display': conditions_display, 'Procedure display':procedures_display, 'Observation display':observations_display, \n",
    "        'Diagnostics Report display':dignotstics_reports_display, 'Immunization display':immunizations_display, 'Medication Orders display':medication_orders_display, 'Careplan display': careplans_display}\n",
    "resource_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_df.replace('', np.nan, inplace=True)\n",
    "resource_df.dropna(axis=0, how='any', subset=['Condition','Procedure', 'Medication Orders', 'Observation'], inplace=True)\n",
    "resource_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df = resource_df[['patient','patient display','Procedure','Procedure display',\n",
    "                            'Medication Orders','Medication Orders display','Observation', \n",
    "                            'Observation display','Condition','Condition display']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67696d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df['Document'] = condition_df['Procedure'].apply(lambda x : ' '.join(x.split(',')))+ ' ' + condition_df['Medication Orders'].apply(lambda x : ' '.join(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [cond for condition in condition_df['Condition'] for cond in condition.split(',') ]\n",
    "conditions_count = dict(sorted(Counter(conditions).items(), key=lambda item:item[1]))\n",
    "topic_candidates = list(conditions_count.keys())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9297cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_bundle = condition_df.Condition.tolist()\n",
    "common = []\n",
    "for case in conditions_bundle:\n",
    "    common.append(np.in1d(np.array(topic_candidates), np.array(case)).sum())\n",
    "unique_indicies = [index for index, count in enumerate(common) if count==1]\n",
    "selected_df = condition_df.iloc[unique_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df['Condition']=selected_df['Condition']\\\n",
    ".map(lambda x: list(set(x.split(',')) & set(topic_candidates))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ba22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [cond for condition in selected_df['Condition'] for cond in condition.split(',') ]\n",
    "conditions_count = dict(sorted(Counter(conditions).items(), key=lambda item:item[1]))\n",
    "conditions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a09580",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.to_csv('data/synthea.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1e59a",
   "metadata": {},
   "source": [
    "# Build the Text-GCN graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0017353",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthea_df = pd.read_csv('data/synthea.csv')\n",
    "# conditions_df = pd.read_csv('../text_gcn_on_fhir/fhirlarge.csv')\n",
    "# synthea_df = conditions_df\n",
    "synthea_df['Condition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a166a9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724 543 109 72\n"
     ]
    }
   ],
   "source": [
    "#train_test_split\n",
    "size = len(synthea_df)\n",
    "# size = len(ngtext)\n",
    "train_size = int(0.75*size)\n",
    "val_size = int(0.1*size)\n",
    "test_size = size - train_size - val_size\n",
    "print(size, train_size, test_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e622db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "shuffled_id = np.arange(size)\n",
    "random.shuffle(shuffled_id)\n",
    "shuffled_document = synthea_df['Document'][shuffled_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e96e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = synthea_df['Condition'].values[shuffled_id]\n",
    "labels_list = list(synthea_df['Condition'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e18eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "word_freq = defaultdict(lambda : 0)\n",
    "word_set = set()\n",
    "for doc_words in shuffled_document:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        word_freq[word] += 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "word_embeddings_dim = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735cd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_list = defaultdict(lambda : [])\n",
    "for index, doc in enumerate(shuffled_document):\n",
    "    appeared = set()\n",
    "    doc_words = doc.split()\n",
    "    for word in doc_words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        word_doc_list[word].append(index)\n",
    "        appeared.add(word)\n",
    "        \n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_id_map[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fad9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ecoding using tfidf\n",
    "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vec.fit_transform(vocab)\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e797b0f",
   "metadata": {},
   "source": [
    "### Feature matrix preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f0080e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_map = {}\n",
    "for word, idd in word_id_map.items():\n",
    "    word_vector_map[word] = tfidf_matrix_array[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c512797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data feature\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "\n",
    "for i, doc in enumerate(shuffled_document[:train_size+val_size]):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = doc.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "#         print(word)\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector) \n",
    "            \n",
    "    data_x.append(doc_vec)\n",
    "x = np.array(data_x)/doc_len        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc185807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data labels\n",
    "y = []\n",
    "for label in labels[:train_size+val_size]:\n",
    "    one_hot = [0 for _ in range(len(labels_list))]\n",
    "    index = labels_list.index(label)\n",
    "    one_hot[index]= 1\n",
    "    y.append(one_hot)\n",
    "    \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcc58661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87a0ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4914e77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 53) 109\n"
     ]
    }
   ],
   "source": [
    "#test data feature\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "\n",
    "\n",
    "for i, doc in enumerate(shuffled_document[-test_size:]):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = doc.split()\n",
    "    doc_len = len(words)\n",
    "    for word in doc_words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "    data_tx.append(doc_vec)\n",
    "tx = np.array(data_tx)/doc_len\n",
    "\n",
    "\n",
    "#test data label\n",
    "ty = [labels_list.index(label) for label in labels[-test_size:]]\n",
    "\n",
    "print(tx.shape, len(ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027e451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 53) 109 (668, 53) 668 668\n"
     ]
    }
   ],
   "source": [
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "data_allx = []\n",
    "\n",
    "for i, doc in enumerate(shuffled_document[:(train_size+val_size)]):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = doc.split()\n",
    "    doc_len = len(words)\n",
    "    for word in doc_words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "    data_allx.append(doc_vec)\n",
    "data_allx = np.array(data_allx)/doc_len\n",
    "allx = np.vstack([data_allx, np.array(word_vectors)])           \n",
    "\n",
    "ally = [labels_list.index(label) for label in labels[:train_size+val_size]]\n",
    "ally.extend([-1 for _ in range(vocab_size)])\n",
    "\n",
    "# ally = np.array(ally)\n",
    "\n",
    "print(tx.shape, len(ty), allx.shape, len(ally), train_size+val_size+vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363a5e5",
   "metadata": {},
   "source": [
    "### Doc word heterogeneous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0918d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307 2307 2307 776 777\n"
     ]
    }
   ],
   "source": [
    "# word co-occurence with context windows\n",
    "window_size = 3\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffled_document:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            \n",
    "word_window_freq = defaultdict(lambda :0)\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for word in window:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        word_window_freq[word]+=1\n",
    "        appeared.add(word)\n",
    "\n",
    "word_pair_count = defaultdict(lambda :0)\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0,i):\n",
    "            word_i = window[i]\n",
    "            word_j = window[j]\n",
    "            word_i_id  = word_id_map[word_i]\n",
    "            word_j_id =  word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            word_pair_count[word_pair_str]+=1\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            word_pair_count[word_pair_str]+=1\n",
    "            \n",
    "            \n",
    "# Word-Word node pmi as weights\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = np.log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size+val_size +i)\n",
    "    col.append(train_size+val_size + j)\n",
    "    weight.append(pmi)\n",
    "    \n",
    "    \n",
    "# doc word frequency\n",
    "doc_word_freq = defaultdict(lambda : 0)\n",
    "\n",
    "for doc_id, doc in enumerate(shuffled_document):\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        doc_word_freq[doc_word_str]+=1\n",
    "        \n",
    "#Word-Doc node train weights\n",
    "for i, doc_words in enumerate(shuffled_document):\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size+val_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size+val_size+ j)\n",
    "        idf = np.log(1.0 * len(shuffled_document) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + val_size + vocab_size + test_size\n",
    "\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "\n",
    "print(len(row),len(col), len(weight), max(row), node_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40442bb2",
   "metadata": {},
   "source": [
    "# Display the word doc hetergenous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77537a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_display_map = {}\n",
    "for code, display in zip(synthea_df['Procedure'],synthea_df['Procedure display']):\n",
    "    codes = code.split(',')\n",
    "    displays = display.split(',')\n",
    "    for c, d in zip(codes, displays):\n",
    "        code_display_map[c]=(d, 'procedure')\n",
    "        \n",
    "for code, display in zip(synthea_df['Medication Orders'],synthea_df['Medication Orders display']):\n",
    "    codes = code.split(',')\n",
    "    displays = display.split(',')\n",
    "    for c, d in zip(codes, displays):\n",
    "        code_display_map[c]=(d, 'medication')\n",
    "        \n",
    "for code, display in zip(synthea_df['Condition'],synthea_df['Condition display']):\n",
    "    code_display_map[code]=(display, 'condition')\n",
    "\n",
    "patient_condition = synthea_df[['patient display', 'Condition display']].values[shuffled_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82ef6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtoname(row, col, adj):\n",
    "    if row < train_size+val_size:\n",
    "        row_id = row\n",
    "        src = patient_condition[row_id][0]\n",
    "        node_type_src = patient_condition[row_id][1]\n",
    "    elif row >= train_size+val_size and row <train_size+val_size+vocab_size :\n",
    "        row_id = row-(train_size+val_size)\n",
    "        src = code_display_map[vocab[row_id]][0]\n",
    "        node_type_src = code_display_map[vocab[row_id]][1]\n",
    "    else:\n",
    "        row_id = row-vocab_size\n",
    "        src = patient_condition[row_id][0]\n",
    "        node_type_src = patient_condition[row_id][1]\n",
    "    col_id = col-(train_size+val_size)\n",
    "    dst = code_display_map[vocab[col_id]][0]\n",
    "    node_type_dst = code_display_map[vocab[col_id]][1]\n",
    "    weight = adj[row][col].item()\n",
    "    \n",
    "    return src, dst, weight, (node_type_src, node_type_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9618a99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acute bronchitis (disorder)', 'Otitis media', 'Prediabetes',\n",
       "       'Viral sinusitis (disorder)'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(patient_condition[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc3b32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "got_net = Network(height='750px', width='100%', bgcolor='#222222', font_color='white', notebook=True)\n",
    "got_net.barnes_hut()\n",
    "# got_net.toggle_physics(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d65d53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_tensor = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "for r,c in zip(row, col):\n",
    "    src, dst, weight, node_type = idtoname(r, c, adj_tensor)\n",
    "    group = {'Acute bronchitis (disorder)':1,'Otitis media':2,'Prediabetes':3, 'Viral sinusitis (disorder)':4, 'procedure':5, 'medication':6, 'condition':7}\n",
    "    got_net.add_node(r, label=src, title=node_type[0], group=group[node_type[0]])\n",
    "    got_net.add_node(c, label=dst, title=node_type[1], group=group[node_type[1]])\n",
    "    got_net.add_edge(r, c, value=weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ea1ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"imgs/fhir.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0c82fdaf0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_net.show('imgs/fhir.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e90c71",
   "metadata": {},
   "source": [
    "## Train on Graph Convolution Network (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a405856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a5169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ally + ty\n",
    "\n",
    "features_merge = np.vstack([allx, tx]).astype('float32')\n",
    "features = sp.csr_matrix(features_merge, dtype=np.float32)\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "features = normalize(features)\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e9d3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.arange(0, node_size)\n",
    "#np.random.shuffle(perm)\n",
    "idx_train = perm[:train_size]\n",
    "idx_val = perm[train_size:train_size + val_size]\n",
    "idx_test= perm[train_size + val_size + vocab_size : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b31d8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3f6df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13f69302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "120ec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f677d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "dropout = 0.5\n",
    "hidden_size =16\n",
    "fastmode = False\n",
    "epochs = 200\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden_size,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be382f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29547426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.5055 acc_train: 0.2081 loss_val: 1.4048 acc_val: 0.3333 time: 0.0667s\n",
      "Epoch: 0002 loss_train: 1.4850 acc_train: 0.2136 loss_val: 1.3734 acc_val: 0.3333 time: 0.0025s\n",
      "Epoch: 0003 loss_train: 1.4600 acc_train: 0.1934 loss_val: 1.3430 acc_val: 0.3333 time: 0.0022s\n",
      "Epoch: 0004 loss_train: 1.3761 acc_train: 0.2468 loss_val: 1.3133 acc_val: 0.3333 time: 0.0018s\n",
      "Epoch: 0005 loss_train: 1.3689 acc_train: 0.2652 loss_val: 1.2850 acc_val: 0.3333 time: 0.0019s\n",
      "Epoch: 0006 loss_train: 1.3332 acc_train: 0.3020 loss_val: 1.2574 acc_val: 0.3611 time: 0.0019s\n",
      "Epoch: 0007 loss_train: 1.2937 acc_train: 0.3683 loss_val: 1.2304 acc_val: 0.6667 time: 0.0017s\n",
      "Epoch: 0008 loss_train: 1.2712 acc_train: 0.4162 loss_val: 1.2046 acc_val: 0.6944 time: 0.0018s\n",
      "Epoch: 0009 loss_train: 1.2277 acc_train: 0.4512 loss_val: 1.1802 acc_val: 0.6806 time: 0.0019s\n",
      "Epoch: 0010 loss_train: 1.1964 acc_train: 0.5064 loss_val: 1.1568 acc_val: 0.5278 time: 0.0018s\n",
      "Epoch: 0011 loss_train: 1.1864 acc_train: 0.4972 loss_val: 1.1342 acc_val: 0.5278 time: 0.0018s\n",
      "Epoch: 0012 loss_train: 1.1656 acc_train: 0.4972 loss_val: 1.1127 acc_val: 0.5139 time: 0.0019s\n",
      "Epoch: 0013 loss_train: 1.0697 acc_train: 0.5359 loss_val: 1.0927 acc_val: 0.4444 time: 0.0018s\n",
      "Epoch: 0014 loss_train: 1.1255 acc_train: 0.5267 loss_val: 1.0741 acc_val: 0.4306 time: 0.0017s\n",
      "Epoch: 0015 loss_train: 1.0555 acc_train: 0.5470 loss_val: 1.0564 acc_val: 0.4306 time: 0.0017s\n",
      "Epoch: 0016 loss_train: 1.0946 acc_train: 0.5488 loss_val: 1.0394 acc_val: 0.4306 time: 0.0017s\n",
      "Epoch: 0017 loss_train: 1.0008 acc_train: 0.5506 loss_val: 1.0229 acc_val: 0.4306 time: 0.0018s\n",
      "Epoch: 0018 loss_train: 1.0202 acc_train: 0.5267 loss_val: 1.0068 acc_val: 0.4306 time: 0.0017s\n",
      "Epoch: 0019 loss_train: 0.9514 acc_train: 0.5414 loss_val: 0.9909 acc_val: 0.4306 time: 0.0018s\n",
      "Epoch: 0020 loss_train: 0.9755 acc_train: 0.5433 loss_val: 0.9745 acc_val: 0.4306 time: 0.0018s\n",
      "Epoch: 0021 loss_train: 0.9561 acc_train: 0.5433 loss_val: 0.9576 acc_val: 0.4306 time: 0.0017s\n",
      "Epoch: 0022 loss_train: 0.9400 acc_train: 0.5378 loss_val: 0.9397 acc_val: 0.4306 time: 0.0021s\n",
      "Epoch: 0023 loss_train: 0.9265 acc_train: 0.5451 loss_val: 0.9209 acc_val: 0.4306 time: 0.0018s\n",
      "Epoch: 0024 loss_train: 0.9311 acc_train: 0.5396 loss_val: 0.9007 acc_val: 0.4444 time: 0.0017s\n",
      "Epoch: 0025 loss_train: 0.8710 acc_train: 0.5506 loss_val: 0.8797 acc_val: 0.4444 time: 0.0018s\n",
      "Epoch: 0026 loss_train: 0.8620 acc_train: 0.5562 loss_val: 0.8576 acc_val: 0.5000 time: 0.0017s\n",
      "Epoch: 0027 loss_train: 0.8018 acc_train: 0.5691 loss_val: 0.8351 acc_val: 0.5000 time: 0.0018s\n",
      "Epoch: 0028 loss_train: 0.8129 acc_train: 0.5856 loss_val: 0.8123 acc_val: 0.5000 time: 0.0018s\n",
      "Epoch: 0029 loss_train: 0.7541 acc_train: 0.6041 loss_val: 0.7890 acc_val: 0.5278 time: 0.0018s\n",
      "Epoch: 0030 loss_train: 0.7527 acc_train: 0.6464 loss_val: 0.7652 acc_val: 0.5278 time: 0.0018s\n",
      "Epoch: 0031 loss_train: 0.7487 acc_train: 0.6354 loss_val: 0.7417 acc_val: 0.5278 time: 0.0018s\n",
      "Epoch: 0032 loss_train: 0.7432 acc_train: 0.6943 loss_val: 0.7187 acc_val: 0.7083 time: 0.0017s\n",
      "Epoch: 0033 loss_train: 0.6918 acc_train: 0.7698 loss_val: 0.6965 acc_val: 0.7361 time: 0.0017s\n",
      "Epoch: 0034 loss_train: 0.6624 acc_train: 0.7901 loss_val: 0.6749 acc_val: 0.7639 time: 0.0018s\n",
      "Epoch: 0035 loss_train: 0.6468 acc_train: 0.8011 loss_val: 0.6538 acc_val: 0.7917 time: 0.0017s\n",
      "Epoch: 0036 loss_train: 0.6238 acc_train: 0.8195 loss_val: 0.6332 acc_val: 0.8194 time: 0.0017s\n",
      "Epoch: 0037 loss_train: 0.6502 acc_train: 0.8103 loss_val: 0.6130 acc_val: 0.8056 time: 0.0017s\n",
      "Epoch: 0038 loss_train: 0.5861 acc_train: 0.8545 loss_val: 0.5930 acc_val: 0.8056 time: 0.0017s\n",
      "Epoch: 0039 loss_train: 0.5872 acc_train: 0.8582 loss_val: 0.5731 acc_val: 0.8750 time: 0.0018s\n",
      "Epoch: 0040 loss_train: 0.6119 acc_train: 0.8195 loss_val: 0.5536 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0041 loss_train: 0.5876 acc_train: 0.8361 loss_val: 0.5345 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0042 loss_train: 0.5686 acc_train: 0.8471 loss_val: 0.5160 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0043 loss_train: 0.5426 acc_train: 0.8656 loss_val: 0.4978 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0044 loss_train: 0.4972 acc_train: 0.8674 loss_val: 0.4800 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0045 loss_train: 0.5031 acc_train: 0.8895 loss_val: 0.4626 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0046 loss_train: 0.4684 acc_train: 0.8950 loss_val: 0.4454 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0047 loss_train: 0.4443 acc_train: 0.8895 loss_val: 0.4288 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0048 loss_train: 0.4275 acc_train: 0.8858 loss_val: 0.4125 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0049 loss_train: 0.4313 acc_train: 0.8895 loss_val: 0.3969 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0050 loss_train: 0.4623 acc_train: 0.8637 loss_val: 0.3819 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0051 loss_train: 0.4335 acc_train: 0.8674 loss_val: 0.3675 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0052 loss_train: 0.3917 acc_train: 0.8858 loss_val: 0.3535 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0053 loss_train: 0.3737 acc_train: 0.8987 loss_val: 0.3399 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0054 loss_train: 0.3680 acc_train: 0.8821 loss_val: 0.3267 acc_val: 0.9722 time: 0.0019s\n",
      "Epoch: 0055 loss_train: 0.3611 acc_train: 0.9190 loss_val: 0.3143 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0056 loss_train: 0.3643 acc_train: 0.9190 loss_val: 0.3023 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0057 loss_train: 0.3538 acc_train: 0.9079 loss_val: 0.2909 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0058 loss_train: 0.3499 acc_train: 0.9153 loss_val: 0.2798 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0059 loss_train: 0.3193 acc_train: 0.9116 loss_val: 0.2692 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0060 loss_train: 0.3174 acc_train: 0.9190 loss_val: 0.2593 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0061 loss_train: 0.3633 acc_train: 0.8803 loss_val: 0.2505 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0062 loss_train: 0.3470 acc_train: 0.8913 loss_val: 0.2416 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0063 loss_train: 0.3016 acc_train: 0.9024 loss_val: 0.2328 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0064 loss_train: 0.3067 acc_train: 0.9042 loss_val: 0.2244 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0065 loss_train: 0.2859 acc_train: 0.9042 loss_val: 0.2169 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0066 loss_train: 0.3010 acc_train: 0.9098 loss_val: 0.2099 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0067 loss_train: 0.2663 acc_train: 0.9190 loss_val: 0.2033 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0068 loss_train: 0.3026 acc_train: 0.9024 loss_val: 0.1972 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0069 loss_train: 0.2638 acc_train: 0.9116 loss_val: 0.1920 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0070 loss_train: 0.2528 acc_train: 0.9208 loss_val: 0.1868 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0071 loss_train: 0.2674 acc_train: 0.9190 loss_val: 0.1819 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0072 loss_train: 0.2729 acc_train: 0.9134 loss_val: 0.1771 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0073 loss_train: 0.2753 acc_train: 0.9245 loss_val: 0.1720 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0074 loss_train: 0.2395 acc_train: 0.9134 loss_val: 0.1671 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0075 loss_train: 0.2498 acc_train: 0.9042 loss_val: 0.1622 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0076 loss_train: 0.2455 acc_train: 0.9134 loss_val: 0.1576 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0077 loss_train: 0.2252 acc_train: 0.9116 loss_val: 0.1534 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0078 loss_train: 0.2166 acc_train: 0.9374 loss_val: 0.1494 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0079 loss_train: 0.2793 acc_train: 0.9134 loss_val: 0.1456 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0080 loss_train: 0.2330 acc_train: 0.9263 loss_val: 0.1419 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0081 loss_train: 0.2016 acc_train: 0.9392 loss_val: 0.1385 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0082 loss_train: 0.2168 acc_train: 0.9300 loss_val: 0.1354 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0083 loss_train: 0.2159 acc_train: 0.9263 loss_val: 0.1324 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0084 loss_train: 0.2001 acc_train: 0.9411 loss_val: 0.1298 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0085 loss_train: 0.2033 acc_train: 0.9374 loss_val: 0.1274 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0086 loss_train: 0.2198 acc_train: 0.9484 loss_val: 0.1249 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0087 loss_train: 0.2141 acc_train: 0.9355 loss_val: 0.1226 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0088 loss_train: 0.2003 acc_train: 0.9558 loss_val: 0.1207 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0089 loss_train: 0.1832 acc_train: 0.9558 loss_val: 0.1190 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0090 loss_train: 0.1970 acc_train: 0.9576 loss_val: 0.1175 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0091 loss_train: 0.2072 acc_train: 0.9392 loss_val: 0.1162 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0092 loss_train: 0.2002 acc_train: 0.9355 loss_val: 0.1150 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0093 loss_train: 0.2063 acc_train: 0.9411 loss_val: 0.1138 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0094 loss_train: 0.2382 acc_train: 0.9319 loss_val: 0.1124 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0095 loss_train: 0.2422 acc_train: 0.9263 loss_val: 0.1106 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0096 loss_train: 0.2086 acc_train: 0.9411 loss_val: 0.1091 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0097 loss_train: 0.2178 acc_train: 0.9448 loss_val: 0.1078 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0098 loss_train: 0.1976 acc_train: 0.9466 loss_val: 0.1067 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0099 loss_train: 0.1957 acc_train: 0.9263 loss_val: 0.1058 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0100 loss_train: 0.2299 acc_train: 0.9227 loss_val: 0.1046 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0101 loss_train: 0.1867 acc_train: 0.9392 loss_val: 0.1035 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0102 loss_train: 0.2207 acc_train: 0.9245 loss_val: 0.1020 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0103 loss_train: 0.1968 acc_train: 0.9319 loss_val: 0.1003 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0104 loss_train: 0.2306 acc_train: 0.9190 loss_val: 0.0985 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0105 loss_train: 0.2117 acc_train: 0.9576 loss_val: 0.0971 acc_val: 0.9722 time: 0.0019s\n",
      "Epoch: 0106 loss_train: 0.1884 acc_train: 0.9429 loss_val: 0.0960 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0107 loss_train: 0.1808 acc_train: 0.9503 loss_val: 0.0950 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0108 loss_train: 0.1744 acc_train: 0.9448 loss_val: 0.0940 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0109 loss_train: 0.1976 acc_train: 0.9392 loss_val: 0.0928 acc_val: 0.9722 time: 0.0019s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0110 loss_train: 0.2084 acc_train: 0.9263 loss_val: 0.0916 acc_val: 0.9722 time: 0.0022s\n",
      "Epoch: 0111 loss_train: 0.2128 acc_train: 0.9337 loss_val: 0.0909 acc_val: 0.9722 time: 0.0019s\n",
      "Epoch: 0112 loss_train: 0.2451 acc_train: 0.9337 loss_val: 0.0907 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0113 loss_train: 0.1746 acc_train: 0.9576 loss_val: 0.0905 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0114 loss_train: 0.1776 acc_train: 0.9558 loss_val: 0.0903 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0115 loss_train: 0.1812 acc_train: 0.9466 loss_val: 0.0898 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0116 loss_train: 0.1626 acc_train: 0.9374 loss_val: 0.0894 acc_val: 0.9722 time: 0.0020s\n",
      "Epoch: 0117 loss_train: 0.1727 acc_train: 0.9540 loss_val: 0.0889 acc_val: 0.9722 time: 0.0020s\n",
      "Epoch: 0118 loss_train: 0.1863 acc_train: 0.9392 loss_val: 0.0885 acc_val: 0.9722 time: 0.0022s\n",
      "Epoch: 0119 loss_train: 0.1936 acc_train: 0.9282 loss_val: 0.0882 acc_val: 0.9722 time: 0.0019s\n",
      "Epoch: 0120 loss_train: 0.1891 acc_train: 0.9429 loss_val: 0.0878 acc_val: 0.9722 time: 0.0020s\n",
      "Epoch: 0121 loss_train: 0.2209 acc_train: 0.9392 loss_val: 0.0870 acc_val: 0.9722 time: 0.0025s\n",
      "Epoch: 0122 loss_train: 0.1639 acc_train: 0.9503 loss_val: 0.0861 acc_val: 0.9722 time: 0.0024s\n",
      "Epoch: 0123 loss_train: 0.2008 acc_train: 0.9466 loss_val: 0.0853 acc_val: 0.9722 time: 0.0024s\n",
      "Epoch: 0124 loss_train: 0.1678 acc_train: 0.9374 loss_val: 0.0845 acc_val: 0.9722 time: 0.0021s\n",
      "Epoch: 0125 loss_train: 0.1851 acc_train: 0.9448 loss_val: 0.0836 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0126 loss_train: 0.1838 acc_train: 0.9429 loss_val: 0.0829 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0127 loss_train: 0.1766 acc_train: 0.9374 loss_val: 0.0825 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0128 loss_train: 0.1882 acc_train: 0.9355 loss_val: 0.0827 acc_val: 0.9861 time: 0.0022s\n",
      "Epoch: 0129 loss_train: 0.2017 acc_train: 0.9208 loss_val: 0.0836 acc_val: 0.9861 time: 0.0020s\n",
      "Epoch: 0130 loss_train: 0.1547 acc_train: 0.9484 loss_val: 0.0845 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0131 loss_train: 0.1901 acc_train: 0.9448 loss_val: 0.0846 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0132 loss_train: 0.1638 acc_train: 0.9466 loss_val: 0.0851 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0133 loss_train: 0.1467 acc_train: 0.9613 loss_val: 0.0858 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0134 loss_train: 0.1538 acc_train: 0.9650 loss_val: 0.0862 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0135 loss_train: 0.1568 acc_train: 0.9613 loss_val: 0.0861 acc_val: 0.9722 time: 0.0016s\n",
      "Epoch: 0136 loss_train: 0.1730 acc_train: 0.9503 loss_val: 0.0855 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0137 loss_train: 0.1820 acc_train: 0.9503 loss_val: 0.0845 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0138 loss_train: 0.1821 acc_train: 0.9650 loss_val: 0.0832 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0139 loss_train: 0.1791 acc_train: 0.9429 loss_val: 0.0821 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0140 loss_train: 0.1711 acc_train: 0.9650 loss_val: 0.0813 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0141 loss_train: 0.1550 acc_train: 0.9650 loss_val: 0.0809 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0142 loss_train: 0.1369 acc_train: 0.9613 loss_val: 0.0807 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0143 loss_train: 0.1609 acc_train: 0.9484 loss_val: 0.0809 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0144 loss_train: 0.1911 acc_train: 0.9374 loss_val: 0.0808 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0145 loss_train: 0.1453 acc_train: 0.9650 loss_val: 0.0807 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0146 loss_train: 0.1896 acc_train: 0.9411 loss_val: 0.0804 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0147 loss_train: 0.1834 acc_train: 0.9448 loss_val: 0.0800 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0148 loss_train: 0.1306 acc_train: 0.9595 loss_val: 0.0796 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0149 loss_train: 0.1942 acc_train: 0.9337 loss_val: 0.0787 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0150 loss_train: 0.1520 acc_train: 0.9650 loss_val: 0.0778 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0151 loss_train: 0.1588 acc_train: 0.9595 loss_val: 0.0770 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0152 loss_train: 0.1363 acc_train: 0.9797 loss_val: 0.0762 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0153 loss_train: 0.1675 acc_train: 0.9466 loss_val: 0.0753 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0154 loss_train: 0.1562 acc_train: 0.9742 loss_val: 0.0744 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0155 loss_train: 0.1360 acc_train: 0.9613 loss_val: 0.0734 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0156 loss_train: 0.1386 acc_train: 0.9576 loss_val: 0.0725 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0157 loss_train: 0.1732 acc_train: 0.9595 loss_val: 0.0717 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0158 loss_train: 0.1534 acc_train: 0.9613 loss_val: 0.0710 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0159 loss_train: 0.1565 acc_train: 0.9687 loss_val: 0.0703 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0160 loss_train: 0.1524 acc_train: 0.9466 loss_val: 0.0696 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0161 loss_train: 0.1484 acc_train: 0.9576 loss_val: 0.0690 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0162 loss_train: 0.1407 acc_train: 0.9669 loss_val: 0.0685 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0163 loss_train: 0.1271 acc_train: 0.9761 loss_val: 0.0680 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0164 loss_train: 0.1336 acc_train: 0.9669 loss_val: 0.0675 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0165 loss_train: 0.1637 acc_train: 0.9558 loss_val: 0.0669 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0166 loss_train: 0.1434 acc_train: 0.9705 loss_val: 0.0663 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0167 loss_train: 0.1303 acc_train: 0.9650 loss_val: 0.0658 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0168 loss_train: 0.1460 acc_train: 0.9484 loss_val: 0.0651 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0169 loss_train: 0.1484 acc_train: 0.9576 loss_val: 0.0646 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0170 loss_train: 0.1387 acc_train: 0.9595 loss_val: 0.0642 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0171 loss_train: 0.1222 acc_train: 0.9632 loss_val: 0.0637 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0172 loss_train: 0.2125 acc_train: 0.9245 loss_val: 0.0636 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0173 loss_train: 0.1281 acc_train: 0.9650 loss_val: 0.0635 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0174 loss_train: 0.1607 acc_train: 0.9466 loss_val: 0.0631 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0175 loss_train: 0.1576 acc_train: 0.9429 loss_val: 0.0625 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0176 loss_train: 0.1558 acc_train: 0.9484 loss_val: 0.0619 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0177 loss_train: 0.1511 acc_train: 0.9650 loss_val: 0.0613 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0178 loss_train: 0.1439 acc_train: 0.9540 loss_val: 0.0606 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0179 loss_train: 0.1243 acc_train: 0.9613 loss_val: 0.0599 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0180 loss_train: 0.1728 acc_train: 0.9466 loss_val: 0.0594 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0181 loss_train: 0.1460 acc_train: 0.9595 loss_val: 0.0591 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0182 loss_train: 0.2003 acc_train: 0.9429 loss_val: 0.0591 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0183 loss_train: 0.1588 acc_train: 0.9595 loss_val: 0.0592 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0184 loss_train: 0.1262 acc_train: 0.9687 loss_val: 0.0595 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0185 loss_train: 0.1810 acc_train: 0.9558 loss_val: 0.0596 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0186 loss_train: 0.1169 acc_train: 0.9669 loss_val: 0.0597 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0187 loss_train: 0.1156 acc_train: 0.9724 loss_val: 0.0596 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0188 loss_train: 0.1588 acc_train: 0.9484 loss_val: 0.0597 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0189 loss_train: 0.1279 acc_train: 0.9521 loss_val: 0.0597 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0190 loss_train: 0.1310 acc_train: 0.9576 loss_val: 0.0598 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0191 loss_train: 0.1604 acc_train: 0.9503 loss_val: 0.0604 acc_val: 0.9861 time: 0.0018s\n",
      "Epoch: 0192 loss_train: 0.1260 acc_train: 0.9687 loss_val: 0.0611 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0193 loss_train: 0.0951 acc_train: 0.9779 loss_val: 0.0617 acc_val: 0.9861 time: 0.0019s\n",
      "Epoch: 0194 loss_train: 0.1620 acc_train: 0.9540 loss_val: 0.0619 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0195 loss_train: 0.1401 acc_train: 0.9521 loss_val: 0.0620 acc_val: 0.9861 time: 0.0016s\n",
      "Epoch: 0196 loss_train: 0.1321 acc_train: 0.9595 loss_val: 0.0619 acc_val: 0.9722 time: 0.0018s\n",
      "Epoch: 0197 loss_train: 0.1361 acc_train: 0.9613 loss_val: 0.0616 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0198 loss_train: 0.1410 acc_train: 0.9558 loss_val: 0.0615 acc_val: 0.9722 time: 0.0017s\n",
      "Epoch: 0199 loss_train: 0.1233 acc_train: 0.9632 loss_val: 0.0611 acc_val: 0.9861 time: 0.0017s\n",
      "Epoch: 0200 loss_train: 0.1247 acc_train: 0.9705 loss_val: 0.0607 acc_val: 0.9861 time: 0.0017s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4382s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "5b4f5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.0795 accuracy= 0.9633\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
